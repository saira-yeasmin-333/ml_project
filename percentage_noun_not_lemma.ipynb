{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7654204,"sourceType":"datasetVersion","datasetId":4462393}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, BertModel\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\n\n# Load your data\ndat = pd.read_csv(\"../input/reviews/balanced_5000_reviews.csv\")\n\n# Preprocess your DataFrame\ndat = dat.drop(columns=['book_id', 'ratings_count', 'review_likes', 'like_share'])\ndat[\"rating_diff\"] = dat[\"user_rating\"] - dat[\"avg_rating\"]\ndat = dat.drop(columns=['avg_rating'])\ndat[\"quote\"] = dat[\"review_text\"].str.contains(\"\\\"\")\ndat[\"review_length\"] = dat[\"review_text\"].str.len()\n# Drop rows with missing 'review_text'\ndat = dat.dropna(subset=['review_text']).reset_index(drop=True)\n\n\n# Initialize the tokenizer and model from the pre-trained BERT base uncased model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')  # Use BertModel\nmodel.eval()  # Set the model to evaluation mode\n\n# Function to get BERT embeddings using BertModel\ndef get_bert_embeddings(sentence):\n    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    # Use the last hidden state as embeddings (alternative to pooler_output)\n    embeddings = outputs.last_hidden_state[:, 0, :].squeeze()  # Get the embeddings of the [CLS] token\n    return embeddings.numpy()  # Convert the tensor to a NumPy array\n\n# Apply the function to the 'review_text' column to get embeddings\ndat['bert_embeddings'] = dat['review_text'].apply(get_bert_embeddings)\n\n# Tokenize, encode, and pad the reviews\nmax_sequence_length = 256  # Maximum sequence length\ntokenized_reviews = [tokenizer.encode(review, add_special_tokens=True, max_length=max_sequence_length, truncation=True, padding='max_length') for review in dat['review_text']]\n\n# Convert the tokenized reviews into tensors\ninput_ids = torch.tensor(tokenized_reviews)\nattention_masks = torch.tensor([[float(i > 0) for i in seq] for seq in input_ids])\n\n# Create a DataLoader\ndataset = TensorDataset(input_ids, attention_masks)\ndataloader = DataLoader(dataset, batch_size=4, shuffle=False)\n\n# Define a function to get sentiment predictions\ndef get_sentiment_predictions(model, dataloader):\n    model.eval()  # Make sure the model is in evaluation mode\n    predictions = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_masks = batch\n            outputs = model(input_ids, attention_mask=attention_masks)\n            logits = outputs[0]\n            probabilities = torch.softmax(logits, dim=1)\n            predictions.extend(probabilities[:, 1].tolist())  # Assuming index 1 corresponds to positive sentiment\n    return predictions\n\n# Get sentiment predictions\nsentiment_predictions = get_sentiment_predictions(model, dataloader)\n\n# Add the predictions to the DataFrame\ndat['sentiment_probabilities'] = sentiment_predictions\n\n# Print the first few rows to verify\n# print(dat.head())\n\n# dat.to_csv(\"filtered_csv_with_sentiment.csv\", index=False)\n\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n# Ensure you've downloaded the necessary NLTK data\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet as wn\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize\n\n\n\n# Apply the function to the 'lemmatized_text' column to get embeddings\ndat['bert_embeddings'] = dat['review_text'].apply(get_bert_embeddings)\n\n# Convert the list of embeddings into a DataFrame where each column represents one dimension of the embeddings\nembeddings_df = pd.DataFrame(dat['bert_embeddings'].tolist())\n\n# Prepare the feature matrix with other features\nX = dat[['user_reviews', 'user_rating', 'days_since_review', 'rating_diff', 'quote', 'review_length','sentiment_probabilities']].copy()\nX = X.apply(pd.to_numeric, errors='coerce')  # Ensure all data is numeric\nX.fillna(0, inplace=True)\n\n# Concatenate the embeddings DataFrame with the other features\nX = pd.concat([X, embeddings_df], axis=1)\n\n# Assuming 'popular' is your target variable\ny = dat['popular']\n\n# Concatenate features and target into a single DataFrame\ndata_with_target = pd.concat([X, y], axis=1)\n\ndef calculate_features(text):\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    num_words = len(tokens)  # Number of words\n    word_lengths = [len(word) for word in tokens]\n    avg_word_len = np.mean(word_lengths) if word_lengths else 0  # Average word length\n\n    # POS tagging\n    pos_tags = pos_tag(tokens)\n    num_verbs = len([word for word, tag in pos_tags if tag.startswith('VB')])  # Count verbs\n    num_nouns = len([word for word, tag in pos_tags if tag.startswith('NN')])  # Count nouns\n    num_adj = len([word for word, tag in pos_tags if tag.startswith('JJ')])  # Count adjectives\n\n    pct_verbs = num_verbs / num_words if num_words > 0 else 0  # Percentage of verbs\n    pct_nouns = num_nouns / num_words if num_words > 0 else 0  # Percentage of nouns\n    pct_adj = num_adj / num_words if num_words > 0 else 0  # Percentage of adjectives\n\n    sentences = nltk.sent_tokenize(text)\n    avg_sent_len = np.mean([len(word_tokenize(sentence)) for sentence in sentences]) if sentences else 0  # Average sentence length\n\n    return num_words, avg_word_len, avg_sent_len, pct_verbs, pct_nouns, pct_adj\n\n# Apply the function to calculate features for each review\nfeatures_df = dat['review_text'].apply(lambda x: calculate_features(x))\nfeatures_df = pd.DataFrame(features_df.tolist(), columns=[\"num_words\", \"avg_word_len\", \"avg_sent_len\", \"pct_verbs\", \"pct_nouns\", \"pct_adj\"])\n\n# Concatenate the new features DataFrame with the original features\nX = pd.concat([X, features_df], axis=1)\n\n# Write the DataFrame to a CSV file\ndata_with_target.to_csv('embed_5000_percentage_notlem.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-24T12:52:07.085412Z","iopub.execute_input":"2024-02-24T12:52:07.085747Z","iopub.status.idle":"2024-02-24T14:14:36.851545Z","shell.execute_reply.started":"2024-02-24T12:52:07.085724Z","shell.execute_reply":"2024-02-24T14:14:36.850545Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0ba5afe077043719b2f62ae2d17a988"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b78a17a3a74d6aae115e1b3c07510d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d69c2156cb174b99a9dca7eb5e95be76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1a7aa406c944b898ba1b5e9b938ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c61f1d1eaff48b798da8d24722ee463"}},"metadata":{}},{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, BertModel\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\ndata_read = data_with_target\n\n# Separate X and y\nX_read = data_read.drop(columns=['popular']).values\ny_read = data_read['popular'].values\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X_read, y_read, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train[:, None], dtype=torch.float32)  # Reshape y to [n_samples, 1]\nX_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\ny_test_tensor = torch.tensor(y_test[:, None], dtype=torch.float32)\n\n# Create TensorDatasets and DataLoaders\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Define the neural network architecture\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.sigmoid(self.fc3(x))\n        return x\n\n# Initialize the model and move it to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('device: ',device)\nmodel = SimpleNN(input_size=X_train_tensor.shape[1]).to(device)\n\n# Define the loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    y_pred = model(X_test_tensor.to(device)).cpu().numpy()\n    y_pred = np.round(y_pred).flatten()\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nroc = roc_auc_score(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'ROC_AUC: {roc}')","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:17:44.721145Z","iopub.execute_input":"2024-02-24T14:17:44.721564Z","iopub.status.idle":"2024-02-24T14:17:54.582984Z","shell.execute_reply.started":"2024-02-24T14:17:44.721531Z","shell.execute_reply":"2024-02-24T14:17:54.582124Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"device:  cpu\nEpoch [1/50], Loss: 0.6249\nEpoch [2/50], Loss: 0.6851\nEpoch [3/50], Loss: 0.6652\nEpoch [4/50], Loss: 0.4932\nEpoch [5/50], Loss: 0.5333\nEpoch [6/50], Loss: 0.3818\nEpoch [7/50], Loss: 0.4227\nEpoch [8/50], Loss: 0.2949\nEpoch [9/50], Loss: 0.3016\nEpoch [10/50], Loss: 0.0549\nEpoch [11/50], Loss: 0.0816\nEpoch [12/50], Loss: 0.1524\nEpoch [13/50], Loss: 0.0337\nEpoch [14/50], Loss: 0.0579\nEpoch [15/50], Loss: 0.0295\nEpoch [16/50], Loss: 0.0562\nEpoch [17/50], Loss: 0.0248\nEpoch [18/50], Loss: 0.0361\nEpoch [19/50], Loss: 0.0032\nEpoch [20/50], Loss: 0.0029\nEpoch [21/50], Loss: 0.0026\nEpoch [22/50], Loss: 0.0043\nEpoch [23/50], Loss: 0.0025\nEpoch [24/50], Loss: 0.0154\nEpoch [25/50], Loss: 0.1565\nEpoch [26/50], Loss: 0.2814\nEpoch [27/50], Loss: 0.0427\nEpoch [28/50], Loss: 0.0100\nEpoch [29/50], Loss: 0.0117\nEpoch [30/50], Loss: 0.0693\nEpoch [31/50], Loss: 0.0035\nEpoch [32/50], Loss: 0.0190\nEpoch [33/50], Loss: 0.0948\nEpoch [34/50], Loss: 0.0013\nEpoch [35/50], Loss: 0.0209\nEpoch [36/50], Loss: 0.0014\nEpoch [37/50], Loss: 0.0009\nEpoch [38/50], Loss: 0.0007\nEpoch [39/50], Loss: 0.4699\nEpoch [40/50], Loss: 0.0188\nEpoch [41/50], Loss: 0.0233\nEpoch [42/50], Loss: 0.0051\nEpoch [43/50], Loss: 0.0018\nEpoch [44/50], Loss: 0.0007\nEpoch [45/50], Loss: 0.0015\nEpoch [46/50], Loss: 0.0014\nEpoch [47/50], Loss: 0.0249\nEpoch [48/50], Loss: 0.0696\nEpoch [49/50], Loss: 0.0018\nEpoch [50/50], Loss: 0.0013\nAccuracy: 0.622\nPrecision: 0.5720524017467249\nRecall: 0.5900900900900901\nROC_AUC: 0.6187860522392896\n","output_type":"stream"}]}]}